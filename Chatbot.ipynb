{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHggdeXjABgp"
      },
      "outputs": [],
      "source": [
        "r#IMPORTING LIBRAIRES\n",
        "\n",
        "import json #to work with JSON data\n",
        "import string #Provides access to several potentially valuable constants\n",
        "import random #implements pseudo-random number generators\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tF #A multidimensional array of elements is represented by this symbol.\n",
        "from tensorflow.keras import Sequential #Sequential groups a linear stack of layers into a tf.keras.Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "nltk.download(\"punkt\")# required package for tokenization\n",
        "nltk.download(\"wordnet\")# word database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC_fxGhpCxX6"
      },
      "outputs": [],
      "source": [
        "#CREATING A JSON FILE\n",
        "\n",
        "##JSON file that lists all the possible outcomes of user interactions with our chatbot\n",
        "##We first need a set of tags that users can use to categorize their queries. These tags include name, age, and many others.\n",
        "##Every new tag would require a unique pattern. This trend identification helps the chatbot train itself on how people would query it, making it more responsive\n",
        "##Then chatbot will return the pre-programmed answers when asked the designated questions.\n",
        "\n",
        "ourData = {\"intents\": [\n",
        "\n",
        "             {\"tag\": \"age\",\n",
        "              \"patterns\": [\"how old are you?\",\"what is your age?\",\"when is your birthday?\"],\n",
        "              \"responses\": [\"I am 2 years old and my birthday was yesterday\"]\n",
        "             },\n",
        "              {\"tag\": \"greeting\",\n",
        "              \"patterns\": [ \"Hi\", \"Hello\", \"Hey\", \"Heya\", \"Namaste\"],\n",
        "              \"responses\": [\"Hi there\", \"Hello\", \"Hi :)\", \"Namaste\"],\n",
        "             },\n",
        "              {\"tag\": \"goodbye\",\n",
        "              \"patterns\": [ \"bye\", \"later\", \"Bye Bye\"],\n",
        "              \"responses\": [\"Bye\", \"take care\", \"Sayonara\"]\n",
        "             },\n",
        "             {\"tag\": \"name\",\n",
        "              \"patterns\": [\"what's your name?\", \"who are you?\", \"what are you called?\", \"How do I call you?\"],\n",
        "              \"responses\": [\"I have no name yet,\" \"You can give me one, and I will appreciate it\"]\n",
        "             }\n",
        "\n",
        "]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peZN09tF3kcP"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfoeJd4j33Ln"
      },
      "outputs": [],
      "source": [
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i46V-aP7H8ge"
      },
      "outputs": [],
      "source": [
        "#PROCESSING DATA\n",
        "\n",
        "##Creating vocabulary of all the terms used in the patterns, list of tag classes, list of all the patterns in the intents file, and all the related tags for each pattern before creating our training data\n",
        "\n",
        "lm = WordNetLemmatizer() #for getting words\n",
        "# lists\n",
        "ourClasses = []\n",
        "newWords = []\n",
        "documentX = []\n",
        "documentY = []\n",
        "# Each intent is tokenized into words and the patterns and their associated tags are added to their respective lists.\n",
        "for intent in ourData[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        ourTokens = nltk.word_tokenize(pattern)\n",
        "        newWords.extend(ourTokens)\n",
        "        documentX.append(pattern)\n",
        "        documentY.append(intent[\"tag\"])\n",
        "\n",
        "\n",
        "    if intent[\"tag\"] not in ourClasses:# add unexisting tags to their respective classes\n",
        "        ourClasses.append(intent[\"tag\"])\n",
        "\n",
        "newWords = [lm.lemmatize(word.lower())\n",
        "for word in newWords if word not in string.punctuation] # set words to lowercase if not in punctuation\n",
        "newWords = sorted(set(newWords))# sorting words\n",
        "ourClasses = sorted(set(ourClasses))# sorting classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXuGnjWj5a5-"
      },
      "outputs": [],
      "source": [
        "print(newWords)\n",
        "print(ourClasses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIHEWjZEIAnG"
      },
      "outputs": [],
      "source": [
        "#DESIGNING A NEURAL NETWORK MODEL\n",
        "\n",
        "##Neural networks only understand numerical values\n",
        "##Transforming our data into numerical values using Bag of Words (BoW) encoding system\n",
        "\n",
        "trainingData = [] # training list array\n",
        "outEmpty = [0] * len(ourClasses)\n",
        "\n",
        "#BOW model\n",
        "for idx, doc in enumerate(documentX):\n",
        "    bagOfwords = []\n",
        "    text = lm.lemmatize(doc.lower())\n",
        "    for word in newWords:\n",
        "        bagOfwords.append(1) if word in text else bagOfwords.append(0)\n",
        "\n",
        "    outputRow = list(outEmpty)\n",
        "    outputRow[ourClasses.index(documentY[idx])] = 1\n",
        "    trainingData.append([bagOfwords, outputRow])\n",
        "\n",
        "random.shuffle(trainingData)\n",
        "trainingData = np.array(trainingData, dtype=object) #coverting our data into an array afterv shuffling\n",
        "\n",
        "x = np.array(list(trainingData[:, 0])) #first trainig phase\n",
        "y = np.array(list(trainingData[:, 1])) #second training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTvqWlEi6pBa"
      },
      "outputs": [],
      "source": [
        "#Designing a neural network to feed our training data\n",
        "##Model will select an appropriate response from the tag associated with a given feature\n",
        "\n",
        "#defining some parameters\n",
        "iShape = (len(x[0]),)\n",
        "oShape = len(y[0])\n",
        "\n",
        "#the deep learning model\n",
        "ourNewModel = Sequential() #sequential model is approprita for a single stack of layers\n",
        "ourNewModel.add(Dense(128, input_shape=iShape, activation=\"relu\")) #Dense fn adds an output layer\n",
        "ourNewModel.add(Dropout(0.5)) #dropout is used to enhance the viz perception of input neurons\n",
        "ourNewModel.add(Dense(64, activation=\"relu\"))\n",
        "ourNewModel.add(Dropout(0.3))\n",
        "ourNewModel.add(Dense(oShape, activation = \"softmax\"))\n",
        "md = tF.keras.optimizers.legacy.Adam(learning_rate=0.01, decay=1e-6) #callable that returns the value to be used with no arguments\n",
        "ourNewModel.compile(loss='categorical_crossentropy',\n",
        "              optimizer=md,\n",
        "              metrics=[\"accuracy\"]) #improves the numerical stability and pushes the computation of the probability distribution into the categorical crossentropy loss function.\n",
        "\n",
        "print(ourNewModel.summary()) #Output model in summary\n",
        "ourNewModel.fit(x, y, epochs=200, verbose=1) #choosing the output as verbose instead of simple, epochs - no. of repetitions for a training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UNjKmJ87FDo"
      },
      "outputs": [],
      "source": [
        "#BUILDING USEFUL FEATURES\n",
        "##For using our chatbot, we must implement the necessary functionality, which will be made easier by building a library of utility functions\n",
        "\n",
        "def ourText(text):\n",
        "  newtkns = nltk.word_tokenize(text)\n",
        "  newtkns = [lm.lemmatize(word) for word in newtkns]\n",
        "  return newtkns\n",
        "\n",
        "def wordBag(text, vocab):\n",
        "  newtkns = ourText(text)\n",
        "  bagOwords = [0] * len(vocab)\n",
        "  for w in newtkns:\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bagOwords[idx] = 1\n",
        "  return np.array(bagOwords)\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  bagOwords = wordBag(text, vocab)\n",
        "  ourResult = ourNewModel.predict(np.array([bagOwords]))[0]\n",
        "  newThresh = 0.2\n",
        "  yp = [[idx, res] for idx, res in enumerate(ourResult) if res > newThresh]\n",
        "\n",
        "  yp.sort(key=lambda x: x[1], reverse=True)\n",
        "  newList = []\n",
        "  for r in yp:\n",
        "    newList.append(labels[r[0]])\n",
        "  return newList\n",
        "\n",
        "def getRes(firstlist, fJson):\n",
        "  tag = firstlist[0]\n",
        "  listOfIntents = fJson[\"intents\"]\n",
        "  for i in listOfIntents:\n",
        "    if i[\"tag\"] == tag:\n",
        "      ourResult = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  return ourResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tCEREHOO8ZOb",
        "outputId": "787a86e3-1560-49cc-d22e-feecf8f0b31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 151ms/step\n",
            "Namaste\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Hi there\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "I am 2 years old and my birthday was yesterday\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "I am 2 years old and my birthday was yesterday\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "I am 2 years old and my birthday was yesterday\n"
          ]
        }
      ],
      "source": [
        "#user will be able to enter a query in a while loop, which will then be cleaned\n",
        "#Next, we use our bag of words model to convert our text into numerical values and make a prediction about which tag in our intent the features most closely represent us\n",
        "\n",
        "while True:\n",
        "    newMessage = input(\"\")\n",
        "    intents = pred_class(newMessage, newWords, ourClasses)\n",
        "    ourResult = getRes(intents, ourData)\n",
        "    print(ourResult)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}